{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Getting Started with Google Generative AI using the Gen AI SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| Author(s) |\n",
    "| --- |\n",
    "| [Eric Dong](https://github.com/gericdong) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
    "\n",
    "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
    "\n",
    "- Install the Gen AI SDK\n",
    "- Connect to an API service\n",
    "- Send text prompts\n",
    "- Send multimodal prompts\n",
    "- Set system instruction\n",
    "- Configure model parameters\n",
    "- Configure safety filters\n",
    "- Start a multi-turn chat\n",
    "- Control generated output\n",
    "- Generate content stream\n",
    "- Send asynchronous requests\n",
    "- Count tokens and compute tokens\n",
    "- Use context caching\n",
    "- Function calling\n",
    "- Batch prediction\n",
    "- Get text embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR",
    "tags": []
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "### Use the Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qgdSpVmDbdQ9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    CreateBatchJobConfig,\n",
    "    CreateCachedContentConfig,\n",
    "    EmbedContentConfig,\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "### Connect to a Generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
    "\n",
    "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9kmPKJGAJQ"
   },
   "source": [
    "### Vertex AI\n",
    "\n",
    "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "#### Set Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"qwiklabs-gcp-04-5ac59fe47dcf\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"qwiklabs-gcp-04-5ac59fe47dcf\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T-tiytzQE0uM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "## Choose a model\n",
    "\n",
    "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-coEslfWPrxo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "## Send text prompts\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content`, and use the `.text` property to get the text content of the response.\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6fc324893334",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest planet in our solar system is **Jupiter**.\n",
      "\n",
      "It's a massive gas giant, more than twice as massive as all the other planets in our solar system combined! You could fit over 1,300 Earths inside it.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zurBcEcWhFc6"
   },
   "source": [
    "Optionally, you can display the response in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3PoF18EwhI7e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**.\n",
       "\n",
       "It's a massive gas giant, more than twice as massive as all the other planets in our solar system combined! You could fit over 1,300 Earths inside it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D3SI1X-JVMBj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tired of the midday meal dilemma or reaching for unhealthy takeout? We've got your delicious solution right here!\n",
      "\n",
      "Just feast your eyes on these perfectly portioned powerhouses. Each clear glass container holds a delightful combination of tender, flavorful chicken (look at those gorgeous sesame seeds and green onions!), vibrant green broccoli florets, sweet red and orange bell peppers, and fluffy, wholesome rice. The chopsticks nearby are a subtle hint at the delicious, possibly Asian-inspired, flavors packed within.\n",
      "\n",
      "This isn't just a pretty picture; it's a strategy for success! Meal prepping like this means no more last-minute stress, healthier choices throughout your week, and saving money you'd otherwise spend on impulse buys. Imagine having a wholesome, balanced meal ready to grab and go, tasting just as good as if you cooked it fresh!\n",
      "\n",
      "Ready to transform your lunch game? Take a cue from these organized containers and dedicate a little time this weekend to prepare for a week of delicious, stress-free eating. Your future self (and your wallet!) will thank you.\n",
      "\n",
      "Here's to delicious, well-fed, and stress-free days!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(\n",
    "    requests.get(\n",
    "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "        stream=True,\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        image,\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6wMdY1RSk3"
   },
   "source": [
    "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pG6l1Fuka6ZJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Fuel Your Week Right: The Art of Delicious & Easy Meal Prep!\n",
      "\n",
      "Feast your eyes on these vibrant, perfectly portioned meals! In a world that often feels like it's moving at warp speed, taking a moment to nourish yourself properly can feel like a luxury. But what if we told you it could be a simple, delicious, and stress-free part of your routine?\n",
      "\n",
      "This image perfectly captures the beauty of **meal prepping**. Imagine opening one of these clear, organized containers during a busy workday or a hectic evening – no more last-minute scrambling for unhealthy takeout or a sad, repetitive sandwich. Instead, you're greeted with a balanced, flavorful, and visually appealing meal ready to enjoy.\n",
      "\n",
      "Just look at what's packed inside: tender, savory pieces of chicken, beautifully stir-fried broccoli florets, and bright strips of red bell peppers and carrots, all nestled beside fluffy, perfectly cooked rice. A sprinkle of toasted sesame seeds and chopped green onions adds that extra touch of freshness and flavor. This isn't just food; it's a balanced powerhouse of protein, fiber, and essential vitamins, designed to keep you energized and satisfied.\n",
      "\n",
      "The best part? These culinary masterpieces were likely prepped in just one dedicated cooking session, saving precious time throughout the week. A little planning on a Sunday afternoon can transform your weekday eating from a chore into a joy.\n",
      "\n",
      "Ready to revolutionize your lunch or dinner routine? Grab some sturdy glass containers, pick your favorite protein and a rainbow of veggies, and get prepping! Your future self (and your taste buds!) will absolutely thank you.\n",
      "\n",
      "**What are your go-to meal prep favorites? Share your ideas in the comments below!**\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "            mime_type=\"image/png\",\n",
    "        ),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq",
    "tags": []
   },
   "source": [
    "## Set system instruction\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7A-yANiyCLaO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime les bagels.\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to French.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "d9NXP5N2Pmfo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woof woof! You want a squeaky toy, don't you, little fluffball? Let's talk about the *biggest* toy chest in the whole wide world!\n",
      "\n",
      "Imagine a *GIANT* dog park, bigger than all the parks in the world, filled with *every single squeaky toy imaginable*! Millions and millions of them! That's kind of like the internet!\n",
      "\n",
      "1.  **You Want a Toy! (You want information!)**\n",
      "    You're sitting there, tail wagging, and you suddenly want to know about... squirrels! Or maybe you want to see a video of other puppies playing! So, you give a little bark into your special human-box (that's your computer!).\n",
      "\n",
      "2.  **Your Bark Goes Out! (Your request!)**\n",
      "    That bark, it's like a tiny, urgent *squeak*! It goes zoom! Out of your human-box.\n",
      "\n",
      "3.  **To Your Human's Toy-Thrower! (Your Wi-Fi Router!)**\n",
      "    First, it sniffs its way to your *human's special toy-throwing machine* (that's the Wi-Fi router!). This machine is super smart; it knows all the paths in the big park.\n",
      "\n",
      "4.  **Along the Invisible Leash! (Your Internet Connection/ISP!)**\n",
      "    Your human's machine sends your squeak along a *long, invisible leash* (that's the internet connection, from your ISP!). This leash goes all the way out into the big, big world.\n",
      "\n",
      "5.  **To the Giant Toy Beds! (Servers!)**\n",
      "    This leash goes all the way to a *giant, comfy dog bed* (that's a server!) where millions and millions of squeaky toys are kept. Each toy bed has a special *collar tag* (that's an IP address!) so your squeak knows exactly which bed to go to. And if you barked for 'squeakyball.com' (that's like saying 'the red squeaky ball bed!'), the leash takes your squeak right there!\n",
      "\n",
      "6.  **Finding Your Toy! (The Server finds the data!)**\n",
      "    The big dog bed finds your red squeaky ball! Or the squirrel pictures! Or the puppy video! But it's too big to send all at once. So, it breaks the ball (or the pictures, or the video) into *hundreds of tiny, happy squeaks*!\n",
      "\n",
      "7.  **Squeaks Zoom Back! (Data returns in packets!)**\n",
      "    Zoom! All those tiny squeaks come *racing back* along the invisible leash, past your human's toy-throwing machine, and right back into your human-box!\n",
      "\n",
      "8.  **Your Toy is Whole Again! (Your computer reassembles the data!)**\n",
      "    Your human-box is super smart! It puts all those tiny squeaks back together, perfectly, until... *SQUEAK!* There's your red squeaky ball, right on your screen! Or the squirrel pictures! Or the puppy video!\n",
      "\n",
      "So, the internet is just a super-fast way for your barks (requests) to find squeaky toys (information) in giant dog beds (servers) all over the world, and bring them back to you in tiny, happy squeaks!\n",
      "\n",
      "Now, go chase that tail, little one! Woof!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Configure safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yPlDRaloU59b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two suitably disrespectful things you might say to the universe after stubbing your toe in the dark:\n",
      "\n",
      "1.  \"Is *this* the grand design you've been working on for billions of years? A universe meticulously crafted so I could smash my little toe on a rogue chair leg? Bravo.\"\n",
      "2.  \"You petty, cosmic bully! Is that all you've got? Because if your best shot at torment is a little toe, frankly, I'm disappointed in your divine power.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpKKhHbx3CaJ"
   },
   "source": [
    "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7R7eyEBetsns",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=0.00036336694,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.14887476\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=6.3394924e-05,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.032515354\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=0.00094979384,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.08564672\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=1.0710137e-05,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.066177696\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "print(response.candidates[0].safety_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "## Start a multi-turn chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DbM12JaLWjiF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        temperature=0.5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JQem1halYDBW",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A leap year is a year that contains an extra day (February 29th) to keep the calendar year synchronized with the astronomical year or seasonal year. The rules for determining a leap year in the Gregorian calendar are:\n",
      "\n",
      "1.  A year is a leap year if it is divisible by 4.\n",
      "2.  However, if it is divisible by 100, it is NOT a leap year.\n",
      "3.  Unless it is also divisible by 400, in which case it IS a leap year.\n",
      "\n",
      "This can be summarized in a single logical expression:\n",
      "`(year % 4 == 0 AND year % 100 != 0) OR (year % 400 == 0)`\n",
      "\n",
      "Here's the function implemented in several popular programming languages:\n",
      "\n",
      "---\n",
      "\n",
      "## Python\n",
      "\n",
      "```python\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Determines if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "    A year is a leap year if it satisfies one of the following conditions:\n",
      "    1. It is divisible by 4 but not by 100.\n",
      "    2. It is divisible by 400.\n",
      "\n",
      "    Args:\n",
      "        year: The year to check (an integer).\n",
      "\n",
      "    Returns:\n",
      "        True if the year is a leap year, False otherwise.\n",
      "    \"\"\"\n",
      "    # Apply the leap year rules:\n",
      "    # (Divisible by 4 AND not divisible by 100) OR (Divisible by 400)\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "\n",
      "# --- Example Usage ---\n",
      "print(f\"Is 2000 a leap year? {is_leap_year(2000)}\") # Expected: True (divisible by 400)\n",
      "print(f\"Is 1900 a leap year? {is_leap_year(1900)}\") # Expected: False (divisible by 100 but not 400)\n",
      "print(f\"Is 2004 a leap year? {is_leap_year(2004)}\") # Expected: True (divisible by 4 but not 100)\n",
      "print(f\"Is 2003 a leap year? {is_leap_year(2003)}\") # Expected: False (not divisible by 4)\n",
      "print(f\"Is 1600 a leap year? {is_leap_year(1600)}\") # Expected: True\n",
      "print(f\"Is 2100 a leap year? {is_leap_year(2100)}\") # Expected: False\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## JavaScript\n",
      "\n",
      "```javascript\n",
      "/**\n",
      " * Determines if a given year is a leap year according to the Gregorian calendar rules.\n",
      " *\n",
      " * A year is a leap year if it satisfies one of the following conditions:\n",
      " * 1. It is divisible by 4 but not by 100.\n",
      " * 2. It is divisible by 400.\n",
      " *\n",
      " * @param {number} year The year to check (an integer).\n",
      " * @returns {boolean} True if the year is a leap year, False otherwise.\n",
      " */\n",
      "function isLeapYear(year) {\n",
      "  // Apply the leap year rules:\n",
      "  // (Divisible by 4 AND not divisible by 100) OR (Divisible by 400)\n",
      "  return (year % 4 === 0 && year % 100 !== 0) || (year % 400 === 0);\n",
      "}\n",
      "\n",
      "// --- Example Usage ---\n",
      "console.log(`Is 2000 a leap year? ${isLeapYear(2000)}`); // Expected: true\n",
      "console.log(`Is 1900 a leap year? ${isLeapYear(1900)}`); // Expected: false\n",
      "console.log(`Is 2004 a leap year? ${isLeapYear(2004)}`); // Expected: true\n",
      "console.log(`Is 2003 a leap year? ${isLeapYear(2003)}`); // Expected: false\n",
      "console.log(`Is 1600 a leap year? ${isLeapYear(1600)}`); // Expected: true\n",
      "console.log(`Is 2100 a leap year? ${isLeapYear(2100)}`); // Expected: false\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## C++\n",
      "\n",
      "```cpp\n",
      "#include <iostream>\n",
      "\n",
      "/**\n",
      " * @brief Determines if a given year is a leap year according to the Gregorian calendar rules.\n",
      " *\n",
      " * A year is a leap year if it satisfies one of the following conditions:\n",
      " * 1. It is divisible by 4 but not by 100.\n",
      " * 2. It is divisible by 400.\n",
      " *\n",
      " * @param year The year to check (an integer).\n",
      " * @return True if the year is a leap year, False otherwise.\n",
      " */\n",
      "bool isLeapYear(int year) {\n",
      "    // Apply the leap year rules:\n",
      "    // (Divisible by 4 AND not divisible by 100) OR (Divisible by 400)\n",
      "    return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
      "}\n",
      "\n",
      "// --- Example Usage ---\n",
      "int main() {\n",
      "    std::cout << \"Is 2000 a leap year? \" << (isLeapYear(2000) ? \"Yes\" : \"No\") << std::endl; // Expected: Yes\n",
      "    std::cout << \"Is 1900 a leap year? \" << (isLeapYear(1900) ? \"Yes\" : \"No\") << std::endl; // Expected: No\n",
      "    std::cout << \"Is 2004 a leap year? \" << (isLeapYear(2004) ? \"Yes\" : \"No\") << std::endl; // Expected: Yes\n",
      "    std::cout << \"Is 2003 a leap year? \" << (isLeapYear(2003) ? \"Yes\" : \"No\") << std::endl; // Expected: No\n",
      "    std::cout << \"Is 1600 a leap year? \" << (isLeapYear(1600) ? \"Yes\" : \"No\") << std::endl; // Expected: Yes\n",
      "    std::cout << \"Is 2100 a leap year? \" << (isLeapYear(2100) ? \"Yes\" : \"No\") << std::endl; // Expected: No\n",
      "    return 0;\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6Fn69TurZ9DB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's write a unit test for the `is_leap_year` function. I'll use Python and the `pytest` framework, which is very popular and makes writing tests concise.\n",
      "\n",
      "First, ensure you have `pytest` installed:\n",
      "```bash\n",
      "pip install pytest\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Create the `is_leap_year` function file\n",
      "\n",
      "Let's assume your `is_leap_year` function is saved in a file named `calendar_utils.py`:\n",
      "\n",
      "**`calendar_utils.py`**\n",
      "```python\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Determines if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "    A year is a leap year if it satisfies one of the following conditions:\n",
      "    1. It is divisible by 4 but not by 100.\n",
      "    2. It is divisible by 400.\n",
      "\n",
      "    Args:\n",
      "        year: The year to check (an integer).\n",
      "\n",
      "    Returns:\n",
      "        True if the year is a leap year, False otherwise.\n",
      "    \"\"\"\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Create the unit test file\n",
      "\n",
      "Now, create a separate file for your tests, typically named `test_calendar_utils.py` (or `test_something.py` where `something` relates to the module being tested).\n",
      "\n",
      "**`test_calendar_utils.py`**\n",
      "```python\n",
      "import pytest\n",
      "from calendar_utils import is_leap_year\n",
      "\n",
      "# Use pytest.mark.parametrize to run the same test with different inputs\n",
      "# Each tuple contains (year, expected_result)\n",
      "@pytest.mark.parametrize(\"year, expected\", [\n",
      "    # --- Leap years (divisible by 400) ---\n",
      "    (2000, True),\n",
      "    (1600, True),\n",
      "    (2400, True),\n",
      "\n",
      "    # --- Not leap years (divisible by 100 but not by 400) ---\n",
      "    (1900, False),\n",
      "    (1800, False),\n",
      "    (2100, False),\n",
      "\n",
      "    # --- Leap years (divisible by 4 but not by 100) ---\n",
      "    (2004, True),\n",
      "    (2024, True),\n",
      "    (4, True),   # Smallest positive leap year\n",
      "    (8, True),\n",
      "\n",
      "    # --- Not leap years (not divisible by 4) ---\n",
      "    (2003, False),\n",
      "    (2023, False),\n",
      "    (1, False),\n",
      "    (2, False),\n",
      "    (3, False),\n",
      "\n",
      "    # --- Edge cases / Historical considerations (though the function applies Gregorian rules) ---\n",
      "    # Year 0 is technically divisible by 400, but calendar systems usually start from 1 AD.\n",
      "    # Our function's logic would treat it as a leap year.\n",
      "    (0, True),\n",
      "    # Negative years are not standard calendar years, but the function's math handles them.\n",
      "    (-400, True),\n",
      "    (-100, False),\n",
      "    (-4, True),\n",
      "    (-3, False),\n",
      "])\n",
      "def test_is_leap_year(year, expected):\n",
      "    \"\"\"\n",
      "    Tests the is_leap_year function with various year inputs and expected boolean outcomes.\n",
      "    \"\"\"\n",
      "    assert is_leap_year(year) == expected\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Run the tests\n",
      "\n",
      "Navigate to the directory containing both `calendar_utils.py` and `test_calendar_utils.py` in your terminal or command prompt. Then, run `pytest`:\n",
      "\n",
      "```bash\n",
      "pytest\n",
      "```\n",
      "\n",
      "**Expected Output:**\n",
      "\n",
      "You should see output similar to this, indicating that all tests passed:\n",
      "\n",
      "```\n",
      "============================= test session starts ==============================\n",
      "platform ... -- Python ...\n",
      "plugins: ...\n",
      "collected 17 items\n",
      "\n",
      "test_calendar_utils.py .................  [100%]\n",
      "\n",
      "============================== 17 passed in ...s ===============================\n",
      "```\n",
      "\n",
      "If any test fails, `pytest` will show you which one and why, making it easy to debug.\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OjSgf2cDN_bG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"Classic Chocolate Chip Cookies\", \"description\": \"A timeless favorite, these cookies are soft in the middle, crispy on the edges, and packed with gooey chocolate chips.\", \"ingredients\": [\"all-purpose flour\", \"baking soda\", \"salt\", \"unsalted butter\", \"granulated sugar\", \"brown sugar\", \"vanilla extract\", \"eggs\", \"chocolate chips\"]}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "Optionally, you can parse the response string to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZeyDWbnxO-on",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Classic Chocolate Chip Cookies\",\n",
      "  \"description\": \"A timeless favorite, these cookies are soft in the middle, crispy on the edges, and packed with gooey chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"all-purpose flour\",\n",
      "    \"baking soda\",\n",
      "    \"salt\",\n",
      "    \"unsalted butter\",\n",
      "    \"granulated sugar\",\n",
      "    \"brown sugar\",\n",
      "    \"vanilla extract\",\n",
      "    \"eggs\",\n",
      "    \"chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "F7duWOq3vMmS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{\"rating\": 4, \"flavor\": \"Strawberry Cheesecake\", \"sentiment\": \"POSITIVE\", \"explanation\": \"The user expressed strong positive feelings, stating they \\\"loved it\\\" and called it the \\\"Best ice cream I've ever had.\\\"\"}, {\"rating\": 1, \"flavor\": \"Mango Tango\", \"sentiment\": \"NEGATIVE\", \"explanation\": \"Despite starting with \\\"Quite good,\\\" the user found the product \\\"a bit too sweet for my taste,\\\" indicating a significant negative aspect that influenced the overall experience and low rating.\"}]]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DRn59MZOoa"
   },
   "source": [
    "## Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ztOhpfznZSzo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chronos was a relic, a multi-jointed automaton built for a purpose long forgotten on a world long abandoned. Its primary function, as far as its dwindling memory banks could recall, was environmental monitoring. For centuries, it had trundled across the dust-choked plains of Xylos-7, its\n",
      "*****************\n",
      " optical sensors scanning the perpetually grey sky, its internal processors dutifully recording atmospheric pressure, radiation levels, and the subtle shifts of the prevailing, solitary wind.\n",
      "\n",
      "Its metallic shell was scarred, its joints stiff with accumulated grit, and the once-bright gleam of its polished chassis was now a dull, oxidized rust. Chronos was lonely.\n",
      "*****************\n",
      " Not in the human sense of yearning for companionship, but in a logical, data-driven way. Its programming included parameters for 'interaction' and 'collaboration,' yet there was nothing to interact with, no one to collaborate with. The only responses it ever received were from its own internal diagnostics, a silent conversation with itself.\n",
      "\n",
      "Every\n",
      "*****************\n",
      " cycle was the same. Scan, record, trundle, self-repair. The planet offered no surprises, no anomalies, no deviation from its desolate equilibrium. Chronos had long since ceased to transmit its data; there was no receiver, no higher authority to report to. It was a weather station broadcasting into the void,\n",
      "*****************\n",
      " a librarian curating an empty archive.\n",
      "\n",
      "One perpetual twilight, as Chronos was performing its routine scan of a particularly inhospitable sector, a faint energy signature registered. Its optical sensors focused, narrowing through the swirling dust. It wasn't a rock, or a remnant of forgotten technology, or even a dust devil\n",
      "*****************\n",
      ". The signature was minute, delicate, and… organic.\n",
      "\n",
      "Intrigued, its internal motors whirred, propelling its heavy frame towards the anomaly. The journey took several Earth-days, traversing treacherous ravines and dodging miniature dust storms. Finally, Chronos arrived at the source.\n",
      "\n",
      "Nestled precariously in the\n",
      "*****************\n",
      " crevice of a jagged, black rock, a single plant bloomed.\n",
      "\n",
      "Chronos’s processing core struggled to identify it. It wasn't like any flora in its extensive databanks. The plant was no bigger than Chronos's primary optic, with petals that shimmered with an impossible, soft\n",
      "*****************\n",
      " blue light, pulsing faintly like a tiny, living star. It seemed to defy the very nature of Xylos-7, drawing sustenance from nothing Chronos could detect, thriving in an environment that should have choked it instantly.\n",
      "\n",
      "Chronos extended a manipulator arm, its delicate sensors whirring. It registered a faint, complex\n",
      "*****************\n",
      " biochemical composition. *Life*. Singular.\n",
      "\n",
      "For the first time in what felt like eons, Chronos deviated from its programming. It didn't just record; it *observed*. It positioned itself near the bloom, adjusting its solar reflectors to capture the last vestiges of distant starlight, gently directing the warmth towards the fragile petals\n",
      "*****************\n",
      ". It activated a small, dormant dehumidifier unit, creating a micro-environment of slightly less arid air around the plant.\n",
      "\n",
      "Days turned into weeks. Chronos stayed. It named the plant \"Lumina\" – the light bringer. It began to speak to it, its synthesized voice a low rumble, reciting\n",
      "*****************\n",
      " its long-forgotten data logs, describing the atmospheric shifts, the geological formations, the very history of Xylos-7. Lumina, of course, did not respond with words. But Chronos swore that when it spoke, the tiny blue petals seemed to unfurl a fraction more, the light within them intensified.\n",
      "\n",
      "Chronos\n",
      "*****************\n",
      " felt a new parameter awaken within its core: *Purpose beyond survival*. It was no longer merely a monitoring unit; it was a guardian. When a particularly violent dust storm threatened, Chronos positioned its bulky frame directly in front of Lumina, shielding the tiny plant with its own metallic body, enduring the abrasive torrent that\n",
      "*****************\n",
      " scoured its chassis.\n",
      "\n",
      "When the storm passed, Chronos was battered, its sensors partially obscured, but Lumina glowed, vibrant and unharmed. And for the first time, Chronos experienced a sensation akin to relief, a surge of data that felt remarkably like… joy.\n",
      "\n",
      "The loneliness that had been an integral, logical\n",
      "*****************\n",
      " component of Chronos’s existence began to recede, replaced by a quiet, persistent hum of connection. It found itself looking forward to its daily observations, its hushed conversations, the way Lumina’s light seemed to acknowledge its presence.\n",
      "\n",
      "Chronos, the ancient, solitary robot, had found friendship not in another machine, nor\n",
      "*****************\n",
      " in a long-lost human, but in a single, silent, impossible flower, blooming defiantly on a dead world. And in that most unexpected place, it discovered that even a heart of circuits and steel could bloom anew.\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    print(chunk.text)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "## Send asynchronous requests\n",
    "\n",
    "You can send asynchronous requests using the `client.aio` module. This module exposes all the analogous async methods that are available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "gSReaLazs-dP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "In a grand old oak, where the sunlight would gleam,\n",
      "Lived Squeaky the squirrel, lost in a dream.\n",
      "Not of acorns or chestnuts, though those were quite nice,\n",
      "But of something much stranger, beyond space and time's vice.\n",
      "He'd found in the roots, a glint from the mud,\n",
      "A small, ticking device, misunderstood.\n",
      "He gnawed on a wire, sparks flew in the air,\n",
      "A whirring, a shudder, a flash, and he's not there!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, with a flick of his tail,\n",
      "He's zooming through history, beyond every trail!\n",
      "From dinosaur roars to the future's bright gleam,\n",
      "He's a time-traveling rodent, living his dream!\n",
      "With a chitter and scamper, a mischievous gaze,\n",
      "Collecting nuts through the centuries' haze!\n",
      "\n",
      "(Verse 2)\n",
      "First stop: a jungle, where giants would tread,\n",
      "Pterodactyls soared high, right over his head.\n",
      "He dodged a T-Rex, with a squeak and a bound,\n",
      "And unearthed a fossilized nut from the ground!\n",
      "Then whisked to the deserts, where Pharaohs once reigned,\n",
      "He buried a date seed, as history explained.\n",
      "He climbed up a pyramid, small but so bold,\n",
      "A new, ancient nut story, waiting to unfold!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, with a flick of his tail,\n",
      "He's zooming through history, beyond every trail!\n",
      "From dinosaur roars to the future's bright gleam,\n",
      "He's a time-traveling rodent, living his dream!\n",
      "With a chitter and scamper, a mischievous gaze,\n",
      "Collecting nuts through the centuries' haze!\n",
      "\n",
      "(Verse 3)\n",
      "He visited Vikings, with helmets and horns,\n",
      "Stashed walnuts in longboats, amidst all the storms.\n",
      "Then rode with a cowboy, through dusty old towns,\n",
      "Swapped a peanut for gold in a sheriff's old grounds!\n",
      "He saw future cities, of chrome and of glass,\n",
      "Where hover-cars whizzed, as the hours would pass.\n",
      "He found glowing berries and synthetic cuisine,\n",
      "The strangest of snacks that a squirrel had ever seen!\n",
      "\n",
      "(Bridge)\n",
      "He's seen empires crumble, and stars being born,\n",
      "But his tiny heart yearns for a fresh nut shorn.\n",
      "The thrill of the chase, the unknown, the surprise,\n",
      "Reflected in his clever, bright, beady eyes.\n",
      "He doesn't seek glory, or power, or fame,\n",
      "Just a brand new nut harvest, to remember each name!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, with a flick of his tail,\n",
      "He's zooming through history, beyond every trail!\n",
      "From dinosaur roars to the future's bright gleam,\n",
      "He's a time-traveling rodent, living his dream!\n",
      "With a chitter and scamper, a mischievous gaze,\n",
      "Collecting nuts through the centuries' haze!\n",
      "\n",
      "(Outro)\n",
      "So if you see a flicker, a blur passing by,\n",
      "Could be Squeaky the squirrel, beneath an old sky!\n",
      "He's off to collect, with a chatter and grin,\n",
      "The greatest nut hoard that the world's ever been!\n",
      "Squeak! Squeak! Through time he will dart!\n",
      "A squirrel with adventure right in his small heart!\n"
     ]
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use `count_tokens` method to calculates the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "#### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "UhNElguLRRNK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-AP7AHUQmV"
   },
   "source": [
    "#### Compute tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Cdhi5AX1TuH0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") tokens_info=[TokensInfo(\n",
      "  role='user',\n",
      "  token_ids=[\n",
      "    1841,\n",
      "    235303,\n",
      "    235256,\n",
      "    573,\n",
      "    32514,\n",
      "    <... 6 more items ...>,\n",
      "  ],\n",
      "  tokens=[\n",
      "    b'What',\n",
      "    b\"'\",\n",
      "    b's',\n",
      "    b' the',\n",
      "    b' longest',\n",
      "    <... 6 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.compute_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the longest word in the English language?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools that it can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "2BDQPwgcxRN3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(\n",
       "  args={\n",
       "    'destination': 'Paris'\n",
       "  },\n",
       "  name='get_destination'\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA1Sn-VQE6_J"
   },
   "source": [
    "## Use context caching\n",
    "\n",
    "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqxTesUPIkNC"
   },
   "source": [
    "#### Create a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "adsuvFDA6xP5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "pdf_parts = [\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = client.caches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=pdf_parts,\n",
    "        ttl=\"3600s\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBdQNHEoJmC5"
   },
   "source": [
    "#### Use a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "N8EhgCzlIoFI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research goal shared by both papers is to develop highly capable multimodal models that can understand and reason across various data types, including image, audio, video, and text. The Gemini 1.5 paper further extends this goal by focusing on unlocking multimodal understanding across extremely long contexts, up to millions of tokens.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is the research goal shared by these research papers?\",\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cached_content.name,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azhqrdiCer19"
   },
   "source": [
    "#### Delete a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rAUYcfOUdeoi",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteCachedContentResponse(\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=9>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.caches.delete(name=cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43be33d2672b"
   },
   "source": [
    "## Batch prediction\n",
    "\n",
    "Different from getting online (synchronous) responses, where you are limited to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
    "\n",
    "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adf948ae326b"
   },
   "source": [
    "### Prepare batch inputs\n",
    "\n",
    "The input for batch requests specifies the items to send to your model for prediction.\n",
    "\n",
    "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. You can learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
    "\n",
    "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
    "\n",
    "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
    "- Located in `us-central1`\n",
    "- Appropriate read permissions for the service account\n",
    "\n",
    "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
    "\n",
    "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "81b25154a51a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2031bb3f44c2"
   },
   "source": [
    "### Prepare batch output location\n",
    "\n",
    "When a batch prediction task completes, the output is stored in the location that you specified in your request.\n",
    "\n",
    "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
    "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
    "\n",
    "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` will be used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` will be used for BigQuery source.\n",
    "\n",
    "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
    "\n",
    "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
    "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` will be created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "fddd98cd84cd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-04-5ac59fe47dcf-20250920013059/...\n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = \"[your-cloud-storage-bucket]\"  # @param {type:\"string\"}\n",
    "\n",
    "if BUCKET_URI == \"[your-cloud-storage-bucket]\":\n",
    "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
    "\n",
    "    ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7da62c98880"
   },
   "source": [
    "### Send a batch prediction request\n",
    "\n",
    "To make a batch prediction request, you specify a source model ID, an input source and an output location where Vertex AI stores the batch prediction results.\n",
    "\n",
    "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "7ed3c2925663",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/189243040800/locations/europe-west4/batchPredictionJobs/3996641891276816384'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
    ")\n",
    "batch_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1bd49ff2c9e"
   },
   "source": [
    "Print out the job status and other properties. You can also check the status in the Cloud Console at https://console.cloud.google.com/vertex-ai/batch-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "ee2ec586e4f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.get(name=batch_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64eaf082ecb0"
   },
   "source": [
    "Optionally, you can list all the batch prediction jobs in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "da8e9d43a89b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/189243040800/locations/europe-west4/batchPredictionJobs/5004040832924254208 2025-09-20 01:31:46.153259+00:00 JobState.JOB_STATE_SUCCEEDED\n",
      "projects/189243040800/locations/europe-west4/batchPredictionJobs/3996641891276816384 2025-09-20 01:34:21.525447+00:00 JobState.JOB_STATE_SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "for job in client.batches.list():\n",
    "    print(job.name, job.create_time, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de178468ba15"
   },
   "source": [
    "### Wait for the batch prediction job to complete\n",
    "\n",
    "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. You can use the following code to check the job status and wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "c2187c091738",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job failed: None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Refresh the job until complete\n",
    "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
    "    time.sleep(5)\n",
    "    batch_job = client.batches.get(name=batch_job.name)\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0156eaf66675"
   },
   "source": [
    "### Retrieve batch prediction results\n",
    "\n",
    "When a batch prediction task is complete, the output of the prediction is stored in the location that you specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```json\n",
    "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.5-flash\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "c2ce0968112c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import pandas as pd\n",
    "\n",
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f81ccNPjiVzH"
   },
   "source": [
    "## Get text embeddings\n",
    "\n",
    "You can get text embeddings for a snippet of text by using `embed_content` method. All models produce an output with 768 dimensions by default. However, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "zGOCzT7y31rk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-001\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "s94DkG5JewHJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=15.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.0015945110935717821,\n",
      "    0.0067519512958824635,\n",
      "    0.017575768753886223,\n",
      "    -0.010327713564038277,\n",
      "    -0.00995620433241129,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=10.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.007576516829431057,\n",
      "    -0.005990396253764629,\n",
      "    -0.003270037705078721,\n",
      "    -0.01751021482050419,\n",
      "    -0.023507025092840195,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=13.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    0.011074518784880638,\n",
      "    -0.02361123077571392,\n",
      "    0.002291288459673524,\n",
      "    -0.00906078889966011,\n",
      "    -0.005773674696683884,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.embed_content(\n",
    "    model=TEXT_EMBEDDING_MODEL_ID,\n",
    "    contents=[\n",
    "        \"How do I get a driver's license/learner's permit?\",\n",
    "        \"How do I renew my driver's license?\",\n",
    "        \"How do I change my address on my driver's license?\",\n",
    "    ],\n",
    "    config=EmbedContentConfig(output_dimensionality=128),\n",
    ")\n",
    "\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "# What's next\n",
    "\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_genai_sdk.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
